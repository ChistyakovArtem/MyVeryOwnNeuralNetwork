{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "outputs": [],
   "source": [
    "class Regularizations:\n",
    "\n",
    "    class L2:\n",
    "        @staticmethod\n",
    "        def reg(a):\n",
    "            return np.mean(a**2)\n",
    "\n",
    "        @staticmethod\n",
    "        def reg_prime(a):\n",
    "            return 2 * a / a.size"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "outputs": [],
   "source": [
    "class Losses:\n",
    "\n",
    "    class MSE:\n",
    "        @staticmethod\n",
    "        def loss(y_true, y_pred):\n",
    "            return np.mean(np.power(y_true-y_pred, 2))\n",
    "\n",
    "        @staticmethod\n",
    "        def loss_prime(y_true, y_pred):\n",
    "            # print(y_true.shape, y_pred.shape)\n",
    "            return 2*(y_pred-y_true) / y_true.size"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "outputs": [],
   "source": [
    "class Activations: # alphabet order\n",
    "\n",
    "    class activation:\n",
    "        @staticmethod\n",
    "        def activation(a):\n",
    "            return a\n",
    "\n",
    "        @staticmethod\n",
    "        def activation_prime(a):\n",
    "            return 1\n",
    "\n",
    "\n",
    "    class ReLU(activation):\n",
    "        @staticmethod\n",
    "        def activation(a):\n",
    "            return np.maximum(a, 0)\n",
    "\n",
    "        @staticmethod\n",
    "        def activation_prime(a):\n",
    "            ret = np.array(1 * (a > 0))\n",
    "            return ret\n",
    "\n",
    "\n",
    "    class sigmoid(activation):\n",
    "        @staticmethod\n",
    "        def activation(a):\n",
    "            return 1 / (1 + np.exp(-a))\n",
    "\n",
    "        @staticmethod\n",
    "        def activation_prime(a):\n",
    "            return a * (1 - a)\n",
    "\n",
    "\n",
    "    class softmax(activation):\n",
    "        @staticmethod\n",
    "        def activation(a):\n",
    "            exp = np.exp(a)\n",
    "            return exp / (0.0001 + np.sum(exp, axis=0)) # mb 0\n",
    "\n",
    "        @staticmethod\n",
    "        def activation_prime(a):\n",
    "            t = np.eye(N=a.shape[0], M=a.shape[1])\n",
    "            return t * a * (1 - a) - (1 - t) * a * a\n",
    "\n",
    "\n",
    "    class stable_softmax(activation):\n",
    "        @staticmethod\n",
    "        def activation(a):\n",
    "            a = a - max(a)\n",
    "            exp = np.exp(a)\n",
    "            return exp / np.sum(exp, axis=1)\n",
    "\n",
    "        # dont know prime\n",
    "\n",
    "\n",
    "    class tanh(activation):\n",
    "        @staticmethod\n",
    "        def activation(a):\n",
    "            return np.tanh(a)\n",
    "\n",
    "        @staticmethod\n",
    "        def activation_prime(a):\n",
    "            return 1 - np.tanh(a)**2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "outputs": [],
   "source": [
    "class Layers:\n",
    "\n",
    "    class DummyLayer:\n",
    "\n",
    "        def __init__(self):\n",
    "            self.input_shape = None\n",
    "            self.output_shape = None\n",
    "\n",
    "        def forward_pass(self, input):\n",
    "            raise NotImplementedError\n",
    "\n",
    "        def backward_pass(self, output):\n",
    "            raise NotImplementedError\n",
    "\n",
    "\n",
    "\n",
    "    class Dense(DummyLayer):\n",
    "\n",
    "        def __init__(self, input_shape=None, output_shape=None, learning_rate=None, reg_const=None, reg_type=None):\n",
    "            super().__init__()\n",
    "            self.input_shape = input_shape\n",
    "            self.output_shape = output_shape\n",
    "\n",
    "            self.input = None\n",
    "            self.output = None\n",
    "\n",
    "            self.learning_rate = learning_rate\n",
    "            self.reg_const = reg_const\n",
    "\n",
    "            self.reg_type = reg_type\n",
    "\n",
    "            if self.reg_type is None:\n",
    "                self.reg_function = None\n",
    "                self.reg_prime = None\n",
    "            else:\n",
    "                self.reg_function = reg_type.reg\n",
    "                self.reg_prime = reg_type.reg_prime\n",
    "\n",
    "            self.features_weights = np.random.rand(input_shape, output_shape) - 0.5\n",
    "            self.bias_weights = np.random.rand(1, output_shape) - 0.5\n",
    "\n",
    "            self.learnable = True\n",
    "\n",
    "        def forward_pass(self, input):\n",
    "            self.input = input\n",
    "            self.output = input @ self.features_weights + self.bias_weights\n",
    "            return self.output\n",
    "\n",
    "        def backward_pass(self, output_error):\n",
    "            input_error = output_error @ self.features_weights.T\n",
    "            weights_error = self.input.T @ output_error + self.reg_const * self.reg_prime(self.features_weights)\n",
    "            bias_error = np.sum(output_error, axis=0)\n",
    "\n",
    "            self.features_weights -= self.learning_rate * weights_error\n",
    "            self.bias_weights -= self.learning_rate * bias_error\n",
    "\n",
    "            return input_error\n",
    "\n",
    "\n",
    "\n",
    "    class Activation(DummyLayer):\n",
    "\n",
    "        def __init__(self, activation_type=Activations.tanh):\n",
    "            super().__init__()\n",
    "            self.activation = activation_type.activation\n",
    "            self.activation_prime = activation_type.activation_prime\n",
    "\n",
    "            self.input = None\n",
    "            self.output = None\n",
    "\n",
    "            self.learnable = False\n",
    "\n",
    "        def forward_pass(self, input):\n",
    "            self.input = input\n",
    "            self.output = self.activation(self.input)\n",
    "            return self.output\n",
    "\n",
    "        def backward_pass(self, output):\n",
    "            return self.activation_prime(self.input) * output\n",
    "\n",
    "\n",
    "\n",
    "    class Conv2d(DummyLayer):\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "\n",
    "    def __init__(self, layers, default_learning_rate=0.01, default_reg_const=0.01, reg_type=Regularizations.L2, loss_class=Losses.MSE):\n",
    "        self.layers = []\n",
    "        for layer in layers:\n",
    "            if layer.learnable:\n",
    "                if layer.learning_rate is None:\n",
    "                    layer.learning_rate = default_learning_rate\n",
    "\n",
    "                if layer.reg_const is None:\n",
    "                    layer.reg_const = default_reg_const\n",
    "\n",
    "                if layer.reg_type is None:\n",
    "                    layer.reg_function = reg_type.reg\n",
    "                    layer.reg_prime = reg_type.reg_prime\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        self.loss = loss_class.loss\n",
    "        self.loss_prime = loss_class.loss_prime\n",
    "\n",
    "    def fit(self, X, y, cnt_epochs=10, cnt_it=10000): # add optimizer\n",
    "        it_for_epoch = cnt_it // cnt_epochs\n",
    "        for i in range(cnt_epochs):\n",
    "            for j in range(it_for_epoch):\n",
    "\n",
    "                output = X\n",
    "                for layer in self.layers:\n",
    "                    output = layer.forward_pass(output)\n",
    "\n",
    "                error = self.loss_prime(y, output)\n",
    "\n",
    "                for layer in reversed(self.layers):\n",
    "                    error = layer.backward_pass(error)\n",
    "\n",
    "            print('epoch %d/%d   error=%f' % (i+1, cnt_epochs, self.loss(y, self.predict(X))))\n",
    "\n",
    "    def predict(self, X):\n",
    "        output = X\n",
    "        for layer in self.layers:\n",
    "            output = layer.forward_pass(output)\n",
    "\n",
    "        return output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 2) (4, 1)\n",
      "epoch 1/10   error=0.192022\n",
      "epoch 2/10   error=0.093582\n",
      "epoch 3/10   error=0.007651\n",
      "epoch 4/10   error=0.002455\n",
      "epoch 5/10   error=0.001340\n",
      "epoch 6/10   error=0.000895\n",
      "epoch 7/10   error=0.000663\n",
      "epoch 8/10   error=0.000523\n",
      "epoch 9/10   error=0.000429\n",
      "epoch 10/10   error=0.000363\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([[0.00120928],\n       [0.97305038],\n       [0.97318436],\n       [0.0020329 ]])"
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([[0, 0],\n",
    "              [0, 1],\n",
    "              [1, 0],\n",
    "              [1, 1]])\n",
    "\n",
    "y = np.array([[0],\n",
    "              [1],\n",
    "              [1],\n",
    "              [0]])\n",
    "\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "nn = NeuralNetwork([\n",
    "    Layers.Dense(2, 2),\n",
    "    Layers.Activation(activation_type=Activations.tanh),\n",
    "    Layers.Dense(2, 1),\n",
    "    Layers.Activation(activation_type=Activations.tanh)\n",
    "], default_learning_rate=0.01, default_reg_const=0)\n",
    "\n",
    "nn.fit(X, y, cnt_epochs=10, cnt_it=30000)\n",
    "\n",
    "nn.predict(X)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}